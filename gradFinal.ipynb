{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "TRAIN_DIR = 'D:/try/train'\n",
    "IMG_SIZE = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append(img)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "book = xlrd.open_workbook('angles.xls')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "labels = [[sheet.cell_value(r, c) for c in range(1,7)] for r in range(1,sheet.nrows)]\n",
    "# Profit !\n",
    "print(len(labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(250, 250, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(6), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "  \n",
    "    num_input_channels = x_tensor.shape.as_list()[3]\n",
    "    num_filters = conv_num_outputs\n",
    "    print(\"x_tensor shape: {} conv_num_outpus: {} conv_ksize: {}\\\n",
    "            conv_strides: {} pool_ksize: {} pool_strides: {}\".format(x_tensor.shape, conv_num_outputs,\n",
    "                                                                      conv_ksize, conv_strides, pool_ksize, pool_strides)) \n",
    "    \n",
    "    shape = [conv_ksize[0], conv_ksize[1], num_input_channels, num_filters]\n",
    "    print(\"shape: \",shape)\n",
    "    weight = tf.Variable(tf.truncated_normal(shape, mean=0, stddev=0.08))\n",
    "    bias = tf.Variable(tf.constant(0.05, shape=[num_filters]))\n",
    "    #bias = tf.Variable(tf.zeros(num_filters))\n",
    "    conv = tf.nn.conv2d(x_tensor, weight, strides=[1,conv_strides[0], conv_strides[1],1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return tf.nn.max_pool(conv, ksize=[1,pool_ksize[0],pool_ksize[1],1], \n",
    "                  strides=[1,pool_strides[0], pool_strides[1], 1],\n",
    "                  padding='SAME')\n",
    "\n",
    "\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "   \n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor)\n",
    "    shape = x_tensor.get_shape()\n",
    "    num_features = shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(x_tensor, [-1, num_features])\n",
    "    print(\"shape: {} num_feautres: {}\".format(shape, layer_flat.get_shape().as_list()[1]))\n",
    "    return layer_flat\n",
    "\n",
    "\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "   \n",
    "    # TODO: Implement Function\n",
    "    print(x_tensor.get_shape())\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], mean=0.0, stddev=0.08))\n",
    "    bias = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "    #bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    tf.Variable(tf.zeros(num_outputs))\n",
    "    fc = tf.matmul(x_tensor, weights) + bias\n",
    "    return tf.nn.relu(fc)\n",
    "\n",
    "\n",
    "\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "\n",
    "    # TODO: Implement Function\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs],mean=0.0,stddev=0.5))\n",
    "    bias = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "    #bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    out = tf.add(tf.matmul(x_tensor, weights),bias)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    conv1 = conv2d1_maxpool(x, 128, [5,5], [2,2], [3,3], [2,2])\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "    conv2 = conv2d1_maxpool(conv1, 256, [3,3], [1,1], [1,1], [2,2])\n",
    "    #conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    #conv3 = conv2d_maxpool(conv2, 128, [5,5], [2,2], [3,3], [2,2])\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl1=flatten(conv2)\n",
    "    print(\"fl1: \",fl1,\" : \",fl1.get_shape().as_list()[1])\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(fl1,500)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1,100)\n",
    "    #tf.nn.dropout(fc1, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fc2,6)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((3624, 4672, 3))\n",
    "y = neural_net_label_input(6)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "   \n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y : label_batch,\n",
    "        keep_prob : keep_probability\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "   \n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={ \n",
    "        x : feature_batch,\n",
    "        y : label_batch,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "         x : feature_batch,\n",
    "        y : label_batch,\n",
    "        keep_prob: 1.0\n",
    "    }) \n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss,validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "book = xlrd.open_workbook('angles.xls')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "labels = [[sheet.cell_value(r, c) for c in range(1,7)] for r in range(1,sheet.nrows)]\n",
    "# Profit !\n",
    "print(len(labels)) \n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for train_data1, labels1 in zip(train_data, labels):\n",
    "           # print (batch_features)\n",
    "           # print ('>>> ', batch_labels)\n",
    "            train_neural_network(sess, optimizer, keep_probability, train_data1,labels1)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, train_data1,labels1, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
